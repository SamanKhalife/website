---
reviewers:
- davidopp
- kevin-wangzefeng
- bsalamat
title: Taints and Tolerations
content_type: concept
weight: 50
---

<!-- overview -->
[_Node affinity_](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) ویژگی‌ای از {{< glossary_tooltip text="پادها" term_id="pod" >}} است که آن‌ها را به یک مجموعه از {{< glossary_tooltip text="گره‌ها" term_id="node" >}} جذب می‌کند (هم به عنوان یک ترجیح و هم به عنوان یک نیاز سخت). _Taints_ عکس این قضیه هستند -- آن‌ها به یک گره اجازه می‌دهند تا مجموعه‌ای از پادها را دفع کند.

_Tolerations_ به پادها اعمال می‌شوند. Tolerations به برنامه‌ریز اجازه می‌دهند پادها با taintهای مطابقت‌دهنده را برنامه‌ریزی کند. Tolerations اجازه‌ی برنامه‌ریزی می‌دهند اما تضمینی برای برنامه‌ریزی ندارند: برنامه‌ریز همچنین [پارامترهای دیگر](/docs/concepts/scheduling-eviction/pod-priority-preemption/) را به عنوان بخشی از عملکرد خود ارزیابی می‌کند.

Taints و tolerations با هم کار می‌کنند تا اطمینان حاصل شود که پادها بر روی گره‌های نامناسب برنامه‌ریزی نمی‌شوند. یک یا چند taint به یک گره اعمال می‌شود؛ این به این معنی است که گره نباید هیچ پادی را که tolerationهای مناسب ندارند بپذیرد.

<!-- body -->

## مفاهیم

شما می‌توانید یک taint را با استفاده از [kubectl taint](/docs/reference/generated/kubectl/kubectl-commands#taint) به یک گره اضافه کنید. به عنوان مثال،

```shell
kubectl taint nodes node1 key1=value1:NoSchedule
```

یک taint روی گره `node1` قرار می‌دهد. این taint دارای کلید `key1`، مقدار `value1` و اثر `NoSchedule` است. این به این معنی است که هیچ پادی نمی‌تواند بر روی `node1` برنامه‌ریزی شود مگر اینکه یک toleration مطابقت‌دهنده داشته باشد.

برای حذف taint اضافه شده توسط دستور بالا، می‌توانید این دستور را اجرا کنید:
```shell
kubectl taint nodes node1 key1=value1:NoSchedule-
```

شما می‌توانید یک toleration را برای یک پاد در PodSpec مشخص کنید. هر دو toleration زیر با taint ایجاد شده توسط خط `kubectl taint` بالا "مطابقت" دارند، و بنابراین یک پاد با هر یک از این tolerationها می‌تواند بر روی `node1` برنامه‌ریزی شود:

```yaml
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"
```

```yaml
tolerations:
- key: "key1"
  operator: "Exists"
  effect: "NoSchedule"
```

برنامه‌ریز پیش‌فرض Kubernetes taints و tolerations را هنگام انتخاب یک گره برای اجرای یک پاد خاص در نظر می‌گیرد. با این حال، اگر شما به صورت دستی `.spec.nodeName` را برای یک پاد مشخص کنید، این عمل برنامه‌ریز را دور می‌زند؛ سپس پاد بر روی گره‌ای که به آن اختصاص داده‌اید بایند می‌شود، حتی اگر taintهای `NoSchedule` بر روی آن گره وجود داشته باشد. اگر این اتفاق بیفتد و گره همچنین دارای taint `NoExecute` باشد، kubelet پاد را از بین می‌برد مگر اینکه یک toleration مناسب تنظیم شده باشد.

در اینجا یک مثال از یک پاد که چند toleration تعریف کرده است آورده شده است:

{{% code_sample file="pods/pod-with-toleration.yaml" %}}

مقدار پیش‌فرض برای `operator` برابر با `Equal` است.

یک toleration زمانی با یک taint "مطابقت" دارد که کلیدها یکسان باشند و اثرات یکسان باشند، و:

* `operator` برابر با `Exists` باشد (در این صورت نباید `value` مشخص شود)، یا
* `operator` برابر با `Equal` باشد و مقادیر باید یکسان باشند.

{{< note >}}

دو مورد خاص وجود دارند:

یک `key` خالی با operator `Exists` با همه کلیدها، مقادیر و اثرات مطابقت دارد که به این معنی است که این toleration همه چیز را تحمل می‌کند.

یک `effect` خالی با تمام اثرات با کلید `key1` مطابقت دارد.

{{< /note >}}

مثال بالا از اثر `NoSchedule` استفاده کرده است. به طور جایگزین، می‌توانید از اثر `PreferNoSchedule` استفاده کنید.

مقادیر مجاز برای فیلد `effect` عبارتند از:

`NoExecute`
: این بر پادهایی که در حال حاضر بر روی گره اجرا می‌شوند تأثیر می‌گذارد به شرح زیر:
  * پادهایی که taint را تحمل نمی‌کنند بلافاصله اخراج می‌شوند
  * پادهایی که taint را بدون مشخص کردن `tolerationSeconds` در مشخصات toleration خود تحمل می‌کنند برای همیشه متصل می‌مانند
  * پادهایی که taint را با مشخص کردن `tolerationSeconds` تحمل می‌کنند به مدت زمان مشخص شده متصل می‌مانند. پس از آن زمان، کنترل‌کننده چرخه زندگی گره پادها را از گره اخراج می‌کند.

`NoSchedule`
: هیچ پاد جدیدی بر روی گره taintشده برنامه‌ریزی نخواهد شد مگر اینکه یک toleration مطابقت‌دهنده داشته باشد. پادهای در حال حاضر در حال اجرای گره **اخراج نمی‌شوند**.

`PreferNoSchedule`
: `PreferNoSchedule` یک نسخه "ترجیحی" یا "نرم" از `NoSchedule` است. صفحه کنترل سعی خواهد کرد از قرار دادن یک پاد که taint را تحمل نمی‌کند بر روی گره اجتناب کند، اما این تضمین‌شده نیست.

شما می‌توانید چندین taint بر روی همان گره و چندین toleration بر روی همان پاد قرار دهید. روش Kubernetes برای پردازش چندین taint و toleration مانند یک فیلتر است: با همه taintهای گره شروع کنید، سپس آن‌هایی را که پاد یک toleration مطابقت‌دهنده دارد نادیده بگیرید؛ taintهای باقی‌مانده که نادیده گرفته نشده‌اند بر روی پاد اثرات نشان داده شده را دارند. به طور خاص،

* اگر حداقل یک taint نادیده‌گرفته نشده با اثر `NoSchedule` وجود داشته باشد، Kubernetes پاد را بر روی آن گره برنامه‌ریزی نخواهد کرد.
* اگر هیچ taint نادیده‌گرفته نشده‌ای با اثر `NoSchedule` وجود نداشته باشد اما حداقل یک taint نادیده‌گرفته نشده با اثر `PreferNoSchedule` وجود داشته باشد، Kubernetes سعی خواهد کرد پاد را بر روی گره برنامه‌ریزی نکند.
* اگر حداقل یک taint نادیده‌گرفته نشده با اثر `NoExecute` وجود داشته باشد، پاد از گره اخراج خواهد شد (اگر در حال حاضر بر روی گره اجرا شود)، و بر روی گره برنامه‌ریزی نخواهد شد (اگر هنوز بر روی گره اجرا نشده باشد).

به عنوان مثال، تصور کنید شما یک گره را به این صورت taint کرده‌اید

```shell
kubectl taint nodes node1 key1=value1:NoSchedule
kubectl taint nodes node1 key1=value1:NoExecute
kubectl taint nodes node1 key2=value2:NoSchedule
```

و یک پاد دو toleration دارد:

```yaml
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"
```

در این حالت، پاد قادر به زمان‌بندی بر روی نود نخواهد بود، زیرا تطبیقی برای سومین تانت وجود ندارد. اما اگر این پاد در حال اجرا بر روی نود باشد، به اجرای خود ادامه خواهد داد زیرا سومین تانت تنها تانت از بین سه تانت موجود است که توسط پاد تحمل نمی‌شود.

به طور معمول، اگر یک تانت با اثر `NoExecute` به یک نود اضافه شود، هر پادی که تطبیق این تانت را نداشته باشد، بلافاصله اخراج خواهد شد، و پادهایی که تطبیق تانت را داشته باشند، هرگز اخراج نخواهند شد. با این حال، یک تحمل با اثر `NoExecute` می‌تواند یک فیلد اختیاری به نام `tolerationSeconds` مشخص کند که تعیین می‌کند پاد تا چه مدت پس از اضافه شدن تانت به نود، در نود باقی خواهد ماند. برای مثال،

```yaml
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"
  tolerationSeconds: 3600
```

به این معنی است که اگر این پاد در حال اجرا باشد و یک تانت مطابق به نود اضافه شود، پاد تا ۳۶۰۰ ثانیه در نود باقی خواهد ماند و سپس اخراج خواهد شد. اگر تانت قبل از این زمان حذف شود، پاد اخراج نخواهد شد.

## موارد استفاده

تانت‌ها و تحمل‌ها یک راه انعطاف‌پذیر برای هدایت پادها از نودها یا اخراج پادهایی که نباید اجرا شوند، هستند. چند مورد استفاده عبارتند از:

* **نودهای اختصاصی**: اگر می‌خواهید مجموعه‌ای از نودها را برای استفاده انحصاری توسط یک گروه خاص از کاربران اختصاص دهید، می‌توانید یک تانت به آن نودها اضافه کنید (مثلاً `kubectl taint nodes nodename dedicated=groupName:NoSchedule`) و سپس یک تحمل متناظر به پادهای آنها اضافه کنید (این کار به راحتی توسط نوشتن یک [کنترلر پذیرش](/docs/reference/access-authn-authz/admission-controllers/) سفارشی انجام می‌شود). پادهایی که تحمل دارند، اجازه خواهند داشت از نودهای تانت‌شده (اختصاصی) استفاده کنند و همچنین از هر نود دیگری در کلاستر استفاده کنند. اگر می‌خواهید نودها را به آنها اختصاص دهید و مطمئن شوید که تنها از نودهای اختصاصی استفاده می‌کنند، باید به همان مجموعه نودها یک برچسب مشابه به تانت اضافه کنید (مثلاً `dedicated=groupName`) و کنترلر پذیرش باید همچنین یک افینیته نود اضافه کند تا پادها فقط بر روی نودهایی که با `dedicated=groupName` برچسب‌گذاری شده‌اند، زمان‌بندی شوند.

* **نودهای با سخت‌افزار ویژه**: در یک کلاستر که یک زیرمجموعه کوچک از نودها دارای سخت‌افزار ویژه‌ای هستند (مثلاً GPUها)، مطلوب است که پادهایی که نیازی به سخت‌افزار ویژه ندارند را از آن نودها دور نگه دارید تا فضای کافی برای پادهایی که به سخت‌افزار ویژه نیاز دارند، باقی بماند. این کار با تانت کردن نودهایی که دارای سخت‌افزار ویژه هستند (مثلاً `kubectl taint nodes nodename special=true:NoSchedule` یا `kubectl taint nodes nodename special=true:PreferNoSchedule`) و افزودن یک تحمل متناظر به پادهایی که از سخت‌افزار ویژه استفاده می‌کنند، انجام می‌شود. همانند مورد استفاده نودهای اختصاصی، احتمالاً آسان‌ترین راه برای اعمال تحمل‌ها، استفاده از یک کنترلر پذیرش سفارشی است. برای مثال، توصیه می‌شود که از [منابع توسعه‌یافته](/docs/concepts/configuration/manage-resources-containers/#extended-resources) برای نمایاندن سخت‌افزار ویژه استفاده کنید، نودهای سخت‌افزار ویژه را با نام منبع توسعه‌یافته تانت کنید و کنترلر پذیرش [ExtendedResourceToleration](/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration) را اجرا کنید. اکنون، به دلیل تانت‌های موجود بر روی نودها، هیچ پادی بدون تحمل مناسب بر روی آنها زمان‌بندی نخواهد شد. اما هنگامی که پادی که درخواست منبع توسعه‌یافته را دارد ارسال می‌شود، کنترلر پذیرش ExtendedResourceToleration به طور خودکار تحمل صحیح را به پاد اضافه خواهد کرد و آن پاد بر روی نودهای سخت‌افزار ویژه زمان‌بندی خواهد شد. این اطمینان حاصل می‌کند که این نودهای سخت‌افزار ویژه برای پادهایی که این سخت‌افزار را درخواست می‌کنند، اختصاص یافته‌اند و نیازی به اضافه کردن دستی تحمل‌ها به پادها نخواهد بود.

* **اخراج‌های مبتنی بر تانت**: رفتار اخراج قابل پیکربندی برای هر پاد زمانی که مشکلات نود وجود دارد، که در بخش بعدی توضیح داده شده است.

## اخراج‌های مبتنی بر تانت

{{< feature-state for_k8s_version="v1.18" state="stable" >}}

کنترلر نود به طور خودکار یک نود را هنگامی که شرایط خاصی درست است، تانت می‌کند. تانت‌های زیر به طور پیش‌فرض وجود دارند:

 * `node.kubernetes.io/not-ready`: نود آماده نیست. این مربوط به وضعیت نود `Ready` است که "`False`" باشد.
 * `node.kubernetes.io/unreachable`: نود از کنترلر نود قابل دسترسی نیست. این مربوط به وضعیت نود `Ready` است که "`Unknown`" باشد.
 * `node.kubernetes.io/memory-pressure`: نود فشار حافظه دارد.
 * `node.kubernetes.io/disk-pressure`: نود فشار دیسک دارد.
 * `node.kubernetes.io/pid-pressure`: نود فشار PID دارد.
 * `node.kubernetes.io/network-unavailable`: شبکه نود در دسترس نیست.
 * `node.kubernetes.io/unschedulable`: نود قابل زمان‌بندی نیست.
 * `node.cloudprovider.kubernetes.io/uninitialized`: وقتی که کیوبلت با یک ارائه‌دهنده ابری "خارجی" شروع می‌شود، این تانت بر روی نود تنظیم می‌شود تا آن را به عنوان غیر قابل استفاده علامت‌گذاری کند. بعد از اینکه یک کنترلر از cloud-controller-manager این نود را مقداردهی اولیه کرد، کیوبلت این تانت را حذف می‌کند.

در صورتی که یک نود باید تخلیه شود، کنترلر نود یا کیوبلت تانت‌های مربوطه را با اثر `NoExecute` اضافه می‌کنند. این اثر به طور پیش‌فرض برای تانت‌های `node.kubernetes.io/not-ready` و `node.kubernetes.io/unreachable` اضافه می‌شود. اگر وضعیت خطا به حالت عادی بازگردد، کیوبلت یا کنترلر نود می‌توانند تانت‌های مربوطه را حذف کنند.

در برخی موارد، هنگامی که نود قابل دسترسی نیست، سرور API نمی‌تواند با کیوبلت روی نود ارتباط برقرار کند. تصمیم به حذف پادها نمی‌تواند به کیوبلت اطلاع داده شود تا زمانی که ارتباط با سرور API دوباره برقرار شود. در این مدت، پادهایی که برای حذف برنامه‌ریزی شده‌اند، ممکن است همچنان بر روی نود جدا شده اجرا شوند.

{{< note >}}
پلان کنترلی نرخ افزودن تانت‌های جدید به نودها را محدود می‌کند. این نرخ‌محدود‌سازی تعداد اخراج‌هایی را که در هنگام غیرقابل دسترسی شدن تعداد زیادی نود به طور همزمان ایجاد می‌شود (برای مثال: در صورت بروز اختلال شبکه) مدیریت می‌کند.
{{< /note >}}

شما می‌توانید `tolerationSeconds` را برای یک پاد مشخص کنید تا مدت زمان ماندن آن پاد بر روی نودی که مشکل دارد یا غیرقابل دسترسی است، تعریف کنید.

برای مثال، شما ممکن است بخواهید یک برنامه با وضعیت محلی زیاد را برای مدت طولانی بر روی نود در صورت وقوع پارتیشن شبکه نگه دارید، به این امید که پارتیشن برطرف شود و بنابراین اخراج پاد جلوگیری شود. تحملی که شما برای آن پاد تنظیم می‌کنید ممکن است به شکل زیر باشد:

```yaml
tolerations:
- key: "node.kubernetes.io/unreachable"
  operator: "Exists"
  effect: "NoExecute"
  tolerationSeconds: 6000
```

{{< note >}}
کوبرنیتز به طور خودکار یک تحمل برای `node.kubernetes.io/not-ready` و `node.kubernetes.io/unreachable` با `tolerationSeconds=300` اضافه می‌کند، مگر اینکه شما یا یک کنترلر به طور صریح آن تحمل‌ها را تنظیم کنید.

این تحمل‌های به طور خودکار اضافه شده باعث می‌شوند که پادها پس از تشخیص یکی از این مشکلات، به مدت 5 دقیقه بر روی نود باقی بمانند.
{{< /note >}}

پادهای [DaemonSet](/docs/concepts/workloads/controllers/daemonset/) با تحمل‌های `NoExecute` برای تانت‌های زیر با `tolerationSeconds` صفر ایجاد می‌شوند:

  * `node.kubernetes.io/unreachable`
  * `node.kubernetes.io/not-ready`

این اطمینان حاصل می‌کند که پادهای Daemon

Set به دلیل این مشکلات هرگز اخراج نمی‌شوند.

## تانت نودها بر اساس شرایط

پلان کنترلی، با استفاده از کنترلر نود، به طور خودکار تانت‌هایی با اثر `NoSchedule` برای [شرایط نود](/docs/concepts/scheduling-eviction/node-pressure-eviction/#node-conditions) ایجاد می‌کند.

زمان‌بند هنگام تصمیم‌گیری‌های زمان‌بندی، تانت‌ها را بررسی می‌کند، نه شرایط نود. این اطمینان حاصل می‌کند که شرایط نود مستقیماً بر زمان‌بندی تأثیر نمی‌گذارند. برای مثال، اگر وضعیت نود `DiskPressure` فعال باشد، پلان کنترلی تانت `node.kubernetes.io/disk-pressure` را اضافه می‌کند و پادهای جدید بر روی نود آسیب‌دیده زمان‌بندی نمی‌شوند. اگر وضعیت نود `MemoryPressure` فعال باشد، پلان کنترلی تانت `node.kubernetes.io/memory-pressure` را اضافه می‌کند.

شما می‌توانید شرایط نود را برای پادهای جدید با افزودن تحمل‌های متناظر نادیده بگیرید. پلان کنترلی همچنین تحمل `node.kubernetes.io/memory-pressure` را بر روی پادهایی که کلاس QoS آنها غیر از `BestEffort` است، اضافه می‌کند. این به این دلیل است که کوبرنیتز پادهایی را که در کلاس‌های QoS `Guaranteed` یا `Burstable` قرار دارند (حتی پادهایی که هیچ درخواست حافظه‌ای ندارند) به عنوان پادهایی که قادر به مقابله با فشار حافظه هستند، در نظر می‌گیرد، در حالی که پادهای جدید `BestEffort` بر روی نود آسیب‌دیده زمان‌بندی نمی‌شوند.

کنترلر DaemonSet به طور خودکار تحمل‌های `NoSchedule` زیر را به همه دیمون‌ها اضافه می‌کند تا از خراب شدن DaemonSet جلوگیری کند.

  * `node.kubernetes.io/memory-pressure`
  * `node.kubernetes.io/disk-pressure`
  * `node.kubernetes.io/pid-pressure` (نسخه 1.14 یا بالاتر)
  * `node.kubernetes.io/unschedulable` (نسخه 1.10 یا بالاتر)
  * `node.kubernetes.io/network-unavailable` (فقط شبکه میزبان)

افزودن این تحمل‌ها اطمینان حاصل می‌کند که سازگاری به عقب حفظ شود. شما همچنین می‌توانید تحمل‌های دلخواه را به DaemonSetها اضافه کنید.

## {{% heading "whatsnext" %}}

* در مورد [اخراج‌های ناشی از فشار نود](/docs/concepts/scheduling-eviction/node-pressure-eviction/) و چگونگی پیکربندی آن بخوانید.
* در مورد [اولویت پاد](/docs/concepts/scheduling-eviction/pod-priority-preemption/) بخوانید.
